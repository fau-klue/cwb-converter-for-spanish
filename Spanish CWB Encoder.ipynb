{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating Spanish Text for the Corpus Workbench\n",
    "\n",
    "This demonstration shows how to prepare and annotate raw text data for the import into to the IMS Open Corpus Workbench (CWB). It will assume you know some basics of Python and NLP.\n",
    "\n",
    "Since working with raw (meaning not-structured) text data can be a tricky task due variations in text formating/encoding and random encounters with messy data, let us first set some parameters for the input and output.\n",
    "\n",
    "## The Input Data\n",
    "\n",
    "For this example project we assume somebody - or something - provided (Spanish) text data in the following format: There are different articles of variing length in a single file, with a headline and a newline separating each article.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Cuba\n",
    "Cuba, oficialmente la República de Cuba, es un país soberano insular del Caribe, asentado en un archipiélago del Mar Caribe. El territorio está organizado en quince provincias y un municipio especial con La Habana como capital y ciudad más poblada.\n",
    "\n",
    "Español cubano \n",
    "El español cubano es la variedad del idioma español empleado en Cuba. Es un subdialecto del español caribeño con pequeñas diferencias regionales, principalmente de entonación y léxico, entre el occidente y el oriente de la isla.\n",
    "```\n",
    "\n",
    "Our task is to extract each article and encode it for CWB.\n",
    "\n",
    "## The Output Data\n",
    "\n",
    "To import corpus data in the IMS Open Corpus Workbench (CWB) we need to encode it using the CWB format. The standard CWB input format is one-word-per-line text, with the surface form in the first column and token-level annotations specified as additional TAB-separated columns. See: http://cwb.sourceforge.net/documentation.php\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "<s>\n",
    "This    DT this\n",
    "is      VB be\n",
    "an      DT a\n",
    "example NN example\n",
    "</s>\n",
    "```\n",
    "\n",
    "Here we see the surface form on the right, a POS Tag and a lemma for each token. Also we include an XML Tag `<s>` to mark sentences.\n",
    "\n",
    "## Program Architecture\n",
    "\n",
    "The program final program will be a commandline tool, in which we feed one input file and it will output the encoded data to *stdout*. \n",
    "\n",
    "This way, we can write leaner code and use simple Linux/Unix commands to scale our tool for more data. Python and spaCy (http://spacy.io/) will do the heavy lifting, the shell will decide what goes in.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "To run this code you need: \n",
    "\n",
    " * Python 3.6\n",
    " * A python virtual environment \n",
    " * spaCy and a Spanish Language Model installed\n",
    " \n",
    "Howto:\n",
    "```\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip3 install spacy\n",
    "python3 -m spacy download es_core_news_md\n",
    "```\n",
    "\n",
    "Enough formalities, let's write some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import sys\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading spacy's Spanish Language Model globally\n",
    "NLP = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now set up some helper functions to structure our script. Always remember: Functions are your friends.\n",
    "\n",
    "Since every document is tuple of headline and body, we will represent each document as such in a datastructure. \n",
    "\n",
    "We will read the input file and pass all its lines into this little function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document(lines_of_document):\n",
    "    \"\"\"\n",
    "    Returns tuple for headline and body.\n",
    "    \"\"\"\n",
    "\n",
    "    Document = collections.namedtuple('Document', ['header', 'body'])\n",
    "    document = Document(body=' '.join(lines_of_document[1:]),\n",
    "                   header=lines_of_document[0])\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example call\n",
    "create_document(['This is the headline', 'I am a line from the body!', 'Me too.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can have multiple articles in a single file, we need to extract each article and pass it into our new function.\n",
    "\n",
    "This function will do that given all lines of a file. We will assume each article is seperated by a newline character and create a list of Document tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_list(lines_of_file):\n",
    "    \"\"\"\n",
    "    Creates a list of Document tuples from all the lines of a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    document = []\n",
    "    documents = []\n",
    "\n",
    "    for line in lines_of_file:\n",
    "            document.append(line.rstrip())\n",
    "            \n",
    "            # Either a newline of the last line\n",
    "            if line == '\\n' or line == lines_of_file[-1]:\n",
    "                documents.append(create_document(document))\n",
    "                document = []\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us read in the actual input file. Along with this Notebook, there is a an example file (2017_11_03_example-text.txt) provided. \n",
    "\n",
    "In a perfect world, working with text data should be straight forward. Everything is properly encoded using Unicode, right? Wrong! \n",
    "\n",
    "Especially data provided from third parties can be messy sometimes. Different Operating Systems, editors and so on can make life hard. Let's write a more robust import to mitigate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './2017_11_03_example-text.txt'\n",
    "filename = filepath.split('/')[-1]\n",
    "\n",
    "try:\n",
    "    encoding = 'utf-8'\n",
    "    input_doc = open(filepath,  encoding=encoding)\n",
    "    lines = input_doc.readlines()\n",
    "except UnicodeDecodeError:\n",
    "    # Windows Encoding\n",
    "    encoding = 'cp1252'    \n",
    "    input_doc = open(filepath,  encoding=encoding)\n",
    "    lines = input_doc.readlines()\n",
    "except UnicodeDecodeError:\n",
    "    # Since we want to pipe stdout, we need to change the target\n",
    "    print('Could not convert file: {}'.format(filename), file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "finally:\n",
    "    input_doc.close()\n",
    "    documents = create_document_list(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of Document tuple can now be easily processed. But first we want to get some metadata for our corpus. \n",
    "\n",
    "Since in this scenario we don't have anything else but the filename to extract metadata, we will do just that. The CQB Encoding allows for XML like tags to structure the document. Thus, we will create a `<text author=\"Arthur\"></text>` tag pair to wrap each document and use XML attributes to describe the metadata.\n",
    "\n",
    "Again we will write some functions to do that, since functions are our friends, remember? This function will take the filename, extract what ever metadata we can get and output a XML tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_header(filename):\n",
    "    \"\"\"\n",
    "    Prints document head with its attributes\n",
    "    Input should be like: 2017_11_03_example-text.txt\n",
    "    \"\"\"\n",
    "\n",
    "    date_list = filename[0:10].split('_')\n",
    "    # Hint: CWB Metadata cannot contain dashes -\n",
    "    name = 'id=\"{}\"'.format(filename[0:-4].replace('-', '_'))\n",
    "    date = 'date=\"{}\"'.format('_'.join(date_list))\n",
    "    year = 'year=\"{}\"'.format(date_list[0])\n",
    "    month = 'month=\"{}\"'.format(date_list[1])\n",
    "    day = 'day=\"{}\"'.format(date_list[2])\n",
    "\n",
    "    header = '<text {} {} {} {} {}>'.format(name, date, year, month, day)\n",
    "\n",
    "    print(header)\n",
    "\n",
    "def print_footer():\n",
    "    \"\"\"\n",
    "    Prints end of document. Just for symmetry.\n",
    "    \"\"\"\n",
    "    print('</text>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_header('2017_11_03_example-text.txt')\n",
    "print_footer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will enable us to import each document with its metadata into the CWB. Hint: CWB metadata shoud not contain dashes, especially the ID (which is used for indexing the documents).\n",
    "\n",
    "Now we can start encoding the body. We will use spaCy to annotate our data and mark each sentence with XML tags.\n",
    "\n",
    "This function will be able to encode both the body and the headline, using the tag parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cwb(document, tag='<s>'):\n",
    "    \"\"\"\n",
    "    Annotates and prints the sentences in CQP format\n",
    "    \"\"\"\n",
    "\n",
    "    doc = NLP(document)\n",
    "    for sentence in doc.sents:    \n",
    "        print(tag)\n",
    "        \n",
    "        sent = NLP(sentence.text)\n",
    "        for token in sent:\n",
    "            print('{word}\\t{pos}\\t{lemma}'.format(\n",
    "                word=token.text,\n",
    "                pos=token.pos_,\n",
    "                lemma=token.lemma_))\n",
    "\n",
    "        print(tag.replace('<', '</'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, lemmatization is a bit wrong at the start of a new sentence. The lemma should be in lower cases. However, we will not fix that here.\n",
    "\n",
    "Finally, putting all the parts together, we now have a small clean main loop to generate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for doc in documents:\n",
    "        print_header(filename)\n",
    "        print_cwb(doc.header, tag='<h1>')\n",
    "        print_cwb(doc.body)\n",
    "        print_footer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the beginning, we want this to be a commandline tool. Therefore, we should add the possibility of passing the input file as a parameter. \n",
    "\n",
    "Since this is a Notebook it's not nice to show you this. However, this would be the snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(description='Convert TXT document into CWB format')\n",
    "\n",
    "parser.add_argument(\n",
    "    '--input',\n",
    "    help='Input TXT file to convert',\n",
    "    dest='input',\n",
    "    required=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale our tool, we can use our Shell to input large amounts of data. This way, our tool doesn't have to worry about that.\n",
    "\n",
    "Example\n",
    "```\n",
    "#!/usr/bin/env sh\n",
    "\n",
    "for f in myfiles/*.txt; do python3 converter.py --input \"$f\" > \"${f}.cwb\";\n",
    "done\n",
    "```\n",
    "\n",
    "The full script and Shell wrapper are provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
